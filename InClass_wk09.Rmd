---
title: "Homework Assignment 9"
author: "Xinyu Yao"
date: "Nov 12, 2019"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---
```{r basicfcn}
# can add quietly=T option to the require() function
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```
#### Q1. Import the dataset into R. Check for basic missing values and normality.
```{r Q1, include=FALSE}
pizza <- read.csv("Pizza.csv")
sum(is.na(pizza))
shapiro.test(pizza$mois)
shapiro.test(pizza$prot)
shapiro.test(pizza$fat)
shapiro.test(pizza$ash)
shapiro.test(pizza$sodium)
shapiro.test(pizza$carb)
shapiro.test(pizza$cal)
```
There is no missing value in the dataset. Shapiro-Wilk test was performed to check normality and the distribution of all of the variables: mois, prot, fat, ash, sodium, carb, and cal, is significantly different from normal distribution. In other words, we cannot assume the normality.

#### Q2. For PCA, which variables will you drop? Get a cleaned up dataset, and call it dfpca. 

For PCA, brand and ID will be dropped.
```{r Q2,include=FALSE}
dfpca <- pizza[ , -which(names(pizza) %in% c("brand","id"))]
```

#### Q3. As we did in class, try both using raw data and centering, obtain the PCA components in the two scenarios. 
```{r Q3,include=FALSE}
pr.out =prcomp(dfpca , center = TRUE, scale =TRUE)
pr.out.nc =prcomp(dfpca , center = FALSE, scale =FALSE) # this is the non-centered, un-normalized data, just for comparison.
#Here are the "loading" vector for the PCA components 
summary(pr.out)
pr.out$rotation
summary(pr.out.nc)
pr.out.nc$rotation
```
#### Q4. How many components are needed to capture 80% of the variance in each case? 
```{r Q4, echo = F}
#Let us plot the cumulation of variance using the sd
pr.var <- (pr.out$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
pr.var.nc <- (pr.out.nc$sdev^2)
pve.nc <- pr.var.nc/sum(pr.var.nc)
plot(cumsum(pve.nc), xlab="Principal Component (non-standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
```

In centered and standardized case, two components are needed to capture 80% of the variance. In non-centered and non-standardized case, only one component is needed.

#### Q5. Make a plot between PC1 and PC2, again for the two cases. 

```{r Q5, echo=F}
biplot(pr.out, scale = 0)
##### 
biplot(pr.out.nc, scale = 0)
```

#### Q6. Before we try PCR, let us perform the basic LM regression the old-fashion way. Build the full model out of the dfpca dataframe with cal as the response variable. What is the R2 value? What are the VIFs? Is this a good model for cal? Afterall, is the cal data calculated from some nutrient’s formula, or are they determined experimentally? What does your analysis suggest?
```{r Q6, include=F}
loadPkg("faraway")
model = lm(cal ~ mois + prot + fat + ash + sodium + carb, data = dfpca)
summary(model)
vif(model)
```
The R^2^ of the model is `r format( summary(model)$r.squared )`. 
VIFs are `r vif(model)`. 
This is not a good model for cal. Though R^2^ is greater but VIF for all variables are larger than 15. 
All the variables are correlated, we cannot determine the nutrient formula from our analysis. They can be measured experimentally. A calorie, like a joule, is a unit of energy. A calorie is equal to the amount of energy per unit mass required to raise the temperature of 1 g of water by 1° C. 
My analysis suggest that there is certain amount of calories per 100 grams	of sample; all variables are negatively correlated to calories; especially, the more water and ash, the less calories in the sample.

#### Q7. Now continue with the dfpca dataframe, build the PCR model, again using both raw and centered versions. Comment on the results in terms of the effect on R2 with different number of components used. 
```{r Q7, include=F}
#install.packages("pls")
loadPkg("pls")
#Again we want to scale our data and "CV" stands for cross validation, which is defaulted to RMSE
pcr.fit=pcr(cal~.,data=dfpca,scale=TRUE,validation ="CV")
summary(pcr.fit)
pcr.nc.fit=pcr(cal~.,data=dfpca,scale=FALSE,validation ="CV")
summary(pcr.nc.fit)
```
```{r, echo=F}
# Plot the R2
validationplot(pcr.fit, val.type = "R2", main = "Standardized")
validationplot(pcr.nc.fit, val.type = "R2", main = "Non-standardized")
```

R^2^ increased with more components used. For both models, with 2 components used, R^2^ are greater than 0.9

#### Q8. In this exercise, do you feel that PCR is a effective tool to build a model with dimension reduction in mind? If so, is the centered-version preferred?

Yes, PCR is a effective tool to build a model with dimension reduction, as for both models, dimension can be reduced to 2 components.The centered-and-scaled-version is preferred. Performing PCA on un-normalized variables will lead to insanely large loadings for variables with high variance. In turn, this will lead to dependence of a principal component on the variable with high variance. This is undesirable. For example, in non-centered case, PC1 depends mostly on mois, however, in centered-and-scaled-case, mois has the least contribution to PC1 and ash has the most contribution to PC1. The variance of mois is ```r var(pizza$mois)``` much higher than the variance of ash ```r var(pizza$ash)```, therefore, un-normalization will lead to dependence of PC on mois, the variable with high variance.